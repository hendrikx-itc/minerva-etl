#!/usr/bin/python
"""
Load tags from txt file
"""
import sys
import os
import re
import argparse
import traceback
import logging
from contextlib import closing
from logging.handlers import RotatingFileHandler
from operator import itemgetter

from configobj import ConfigObj

from minerva.db import connect

from minerva.directory.helpers import get_entitytype

from minerva.directory.tag import NoSuchTagError, tag_entities, flush_tag_links, \
    get_tag, create_tag_group, create_tag
from minerva.util import parse_size
from minerva.db.util import create_copy_from_file, create_temp_table, drop_table

CONFIG_DEFAULTS = """\
db_url = postgresql://minerva:minerva@localhost/minerva
log_directory = /var/log/minerva
log_filename = load-tags.log
log_rotation_size = 10MB
log_level = INFO"""

package_name = "minerva"
script_name = os.path.basename(__file__)
config_file = "{}.conf".format(script_name)



def main():
    parser = argparse.ArgumentParser(
        description="Load tags from file")

    parser.add_argument("file", metavar="FILE", nargs='*',
        help="file containing entity aliases used for tagging")

    parser.add_argument("-t", "--entity-type", default="cell",
        help="name of the entity type (default 'cell')")

    parser.add_argument("-a", "--alias-type", default="name",
        help= "name of alias type (default 'name')")

    parser.add_argument("-c", "--configfile", dest="configfile",
        default="/etc/minerva/load_tags.conf", help="the path to the config file")

    parser.add_argument("-v", "--verbose", action="store_true", default=False,
        help="verbose output")

    parser.add_argument("-f", "--flush", action="store_true", default=False,
        help="flush current tags with specified name")

    parser.add_argument("--generate-configfile", dest="generate_configfile",
        default=False, action="store_true", help="generate base config file")

    args = parser.parse_args()

    if args.generate_configfile:
        print(CONFIG_DEFAULTS)
        return 0

    config = ConfigObj(args.configfile)

    setup_logging(args.verbose)
    setup_file_logging(config["log_directory"], config["log_filename"],
        config["log_rotation_size"], config["log_level"])

    try:
        with closing(connect(config["db_url"])) as conn:
            entitytype = get_entitytype(conn, args.entity_type)

            for file_path in args.file:
                load_tags(conn, entitytype, args.alias_type, file_path, args.flush)
    except Exception:
        traceback.print_exc()
        return 1

    return 0


def load_tags(conn, entitytype, alias_type_name, file_path, flush):
    _directory, file_name = os.path.split(file_path)
    name, _ext = os.path.splitext(file_name)

    logging.info("loading '{}' tags with name '{}' from {}".format(entitytype.name, name, file_path))

    try:
        tag = get_tag(conn, name)
    except NoSuchTagError:

        tag_group = create_tag_group(
            conn, "default", False)

        tag = create_tag(conn, name, tag_group)
        logging.info("created tag '{}'".format(name))
    else:
        if flush:
            logging.info("flushing tag '{}'".format(name))

            flush_tag_links(conn, name)

    with open(file_path, 'rt') as tags_file:
        aliases = [line.rstrip() for line in tags_file]

    unique_aliases = list(set(aliases))

    entity_ids = aliases_to_entity_ids(conn, unique_aliases, "name", "Cell")

    tags = [(entity_id, name) for entity_id in entity_ids]

    tag_entities(conn, tags)

    conn.commit()


def aliases_to_entity_ids(conn, aliases, aliastype_name, entitytype_name):
    tmp_table_name = store_aliases_in_temp_table(conn, [(a,)for a in aliases])

    query = (
        "SELECT a.entity_id FROM {} tmp "
        "JOIN directory.alias a ON a.name = tmp.alias "
        "JOIN directory.aliastype at ON at.id = a.type_id "
        "JOIN directory.entity e ON e.id = a.entity_id "
        "JOIN directory.entitytype et ON et.id = e.entitytype_id "
        "WHERE at.name = %s AND et.name = %s").format(tmp_table_name)

    args = (aliastype_name, entitytype_name)

    with closing(conn.cursor()) as cursor:
        cursor.execute(query, args)

        rows = cursor.fetchall()

    drop_table(conn, tmp_table_name)

    return [entity_id for entity_id, in rows]


def store_aliases_in_temp_table(conn, aliases):
    tmp_table_name = "tmp_aliases"
    columns = [
        ("alias", "varchar"),
    ]
    column_names = [col_name for col_name, col_type in columns]
    sql_columns = ["{} {}".format(*column) for column in columns]

    copy_from_file = create_copy_from_file(aliases, ('s',))
    create_temp_table(conn, tmp_table_name, sql_columns)

    with closing(conn.cursor()) as cursor:
        cursor.copy_from(copy_from_file, tmp_table_name, columns=column_names)

    return tmp_table_name


def setup_logging(verbose):
    root_logger = logging.getLogger("")

    if verbose:
        handler = logging.StreamHandler(sys.stdout)

        root_logger.addHandler(handler)

    root_logger.setLevel(logging.INFO)


def setup_file_logging(directory, filename, rotation_size, level):
    """
    Setup rotating file logging.
    """
    level_map = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL}

    max_log_size = parse_size(rotation_size)

    filepath = os.path.join(directory, filename)
    handler = RotatingFileHandler(filepath, maxBytes=max_log_size, backupCount=5)
    handler.setLevel(level_map[level])

    formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    handler.setFormatter(formatter)

    rootlogger = logging.getLogger("")
    rootlogger.setLevel(level_map[level])
    rootlogger.addHandler(handler)


if __name__ == "__main__":
    sys.exit(main())
